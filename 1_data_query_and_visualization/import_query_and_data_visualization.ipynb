{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1363ea85-247b-4f1e-9607-670d63c335f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI e Aprendizado de Máquina no Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837cfb5b-ddd1-4a1c-b940-b665fc707cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9f3dca-38e2-4802-a3eb-3cc9b33d2254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Link de estudo:\n",
    "\n",
    "* [docs databricks](https://docs.databricks.com/aws/pt/getting-started/quick-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ab74fd6-c9a4-47d9-80c4-eac96eecb6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Consultar e visualizar dados no Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77847a0e-fe49-4008-9b7e-14dbc2f65f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Carregado minhas credenciais do Databricks, com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Só executar quando estiver trabalhando localmente:\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"🔗 Carregado minhas credenciais do Databricks, com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d134c85-70b5-445b-86f9-b3962df56d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conectado ao Databricks com sucesso!\n",
      "🔧 Versão Spark: 4.0.0\n",
      "📦 DBUtils configurado e pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession # Esta classe é usada para conectar ao Databricks remotamente\n",
    "from pyspark.dbutils import DBUtils # Para usar dbutils localmente\n",
    "\n",
    "spark = DatabricksSession.builder.remote(host=databricks_host, token=databricks_token, cluster_id=databricks_cluster_id).getOrCreate()\n",
    "\n",
    "# Criar instância do DBUtils para usar dbutils localmente:\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "print(\"✅ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"🔧 Versão Spark: {spark.version}\")\n",
    "print(f\"📦 DBUtils configurado e pronto para uso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd0f4b5-4ff4-4812-8e75-828cc1104fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Esta célula deve ser executada apenas quando estiver trabalhando localmente:\n",
    "\n",
    "#databricks_cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "#print(f\"Databricks Cluster ID: {databricks_cluster_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87de0a2f-026e-4449-ae60-36809862605f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, trip_distance: double, fare_amount: double, pickup_zip: int, dropoff_zip: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consultando a seguinte Tabela \"trips\" que se encontra no 'samples' do Databricks:\n",
    "taxi_df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "\n",
    "display(taxi_df) # Pode usar, também ---> display(taxi_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.connect.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa9480d-c83a-4218-92cb-dc7d12b179e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚕 === DADOS DA TABELA NYC TAXI TRIPS ===\n",
      "📊 Total de registros (quantidade de dados/linhas): 21,932\n",
      "📋 Número de colunas: 6\n",
      "🏷️ O nomes das colunas são: tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance, fare_amount, pickup_zip, dropoff_zip\n",
      "\n",
      "📋 ESTRUTURA DA TABELA:\n",
      "root\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_zip: integer (nullable = true)\n",
      " |-- dropoff_zip: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exibir informações básicas da tabela\n",
    "print(\"🚕 === DADOS DA TABELA NYC TAXI TRIPS ===\")\n",
    "print(f\"📊 Total de registros (quantidade de dados/linhas): {taxi_df.count():,}\")\n",
    "print(f\"📋 Número de colunas: {len(taxi_df.columns)}\")\n",
    "print(f\"🏷️ O nomes das colunas são: {', '.join(taxi_df.columns)}\")\n",
    "\n",
    "print(\"\\n📋 ESTRUTURA DA TABELA:\")\n",
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676f6e36-6dbd-4b09-bd05-361d4203ebdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 === PRIMEIRAS 7 LINHAS DA TABELA ===\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "|2016-02-13 21:47:53 |2016-02-13 21:57:15  |1.4          |8.0        |10103     |10110      |\n",
      "|2016-02-13 18:29:09 |2016-02-13 18:37:23  |1.31         |7.5        |10023     |10023      |\n",
      "|2016-02-06 19:40:58 |2016-02-06 19:52:32  |1.8          |9.5        |10001     |10018      |\n",
      "|2016-02-12 19:06:43 |2016-02-12 19:20:54  |2.3          |11.5       |10044     |10111      |\n",
      "|2016-02-23 10:27:56 |2016-02-23 10:58:33  |2.6          |18.5       |10199     |10022      |\n",
      "|2016-02-13 00:41:43 |2016-02-13 00:46:52  |1.4          |6.5        |10023     |10069      |\n",
      "|2016-02-18 23:49:53 |2016-02-19 00:12:53  |10.4         |31.0       |11371     |10003      |\n",
      "+--------------------+---------------------+-------------+-----------+----------+-----------+\n",
      "only showing top 7 rows\n"
     ]
    }
   ],
   "source": [
    "# Usando o método show() para exibir: \n",
    "print(\"🔍 === PRIMEIRAS 7 LINHAS DA TABELA ===\")\n",
    "taxi_df.show(7, truncate=False) # Melhor usar \"False\" porque não vai curtar os textos longos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 O shape da tabela é: 21932 linhas e 6 colunas\n"
     ]
    }
   ],
   "source": [
    "# Para saber o shape, fazemos no spark de diferente forma, assim:\n",
    "num_linhas = taxi_df.count()\n",
    "num_colunas = len(taxi_df.columns)\n",
    "\n",
    "print(f\"📊 O shape da tabela é: {num_linhas} linhas e {num_colunas} colunas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17dddf4c-ea9e-4f9a-854e-25c111c60e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A seguir vou consultar meus dados `CSV` que tenho no meu `volume` dentro do `Catalog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac813c1-30aa-44fa-9b85-998053454b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando a função display:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, City: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o método show:\n",
      "+-------+---+-----------+\n",
      "|Name   |Age|City       |\n",
      "+-------+---+-----------+\n",
      "|Alice  |25 |New York   |\n",
      "|Bob    |17 |Los Angeles|\n",
      "|Charlie|35 |Chicago    |\n",
      "|Diana  |16 |Houston    |\n",
      "|Edward |45 |Phoenix    |\n",
      "+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_volume = spark.read.csv(\"/Volumes/workspace/default_eddy/volumeeddy-tmp-sampledata/sample_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"Usando a função display:\")\n",
    "display(df_volume)\n",
    "\n",
    "print(\"Usando o método show:\")\n",
    "df_volume.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9312e00c-670f-4e6b-84f7-e02fe57aa632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importar e visualizar dados do CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d7004e-11c5-4069-bbcf-7d5be7584902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Primeiro definimos, criamos o necessário para poder salvar nossos dados `\"nome de bebês\"` em nosso volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1903e5-5f64-4672-9229-f05944cc5935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O caminho completo da tabela é: catalogeddy.catalogeddy_schema\n",
      "O caminho completo do volume é:/Volumes/catalogeddy/catalogeddy_schema/catalogeddy_volume\n"
     ]
    }
   ],
   "source": [
    "catalog = \"catalogeddy\"\n",
    "schema = \"catalogeddy_schema\"\n",
    "volume = \"catalogeddy_volume\"\n",
    "download_url = \"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv\" # São dados de nome de bebês\n",
    "file_name = \"baby_eddy.csv\"\n",
    "table_name = \"tabel_baby_eddy\"\n",
    "path_volume = \"/Volumes/\" + catalog + \"/\" + schema + \"/\" + volume\n",
    "path_table = catalog + \".\" + schema\n",
    "\n",
    "print(f\"O caminho completo da tabela é: {path_table}\") \n",
    "print(f\"O caminho completo do volume é:{path_volume}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c945ad-edf9-44c4-9d8b-44ca68dc4396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Este comando copia os dados baixados e salva dentro do volume criado anteriormente:\n",
    "#dbutils.fs.cp(f\"{download_url}\", f\"{path_volume}\" + \"/\" + f\"{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f260fb54-8443-4f79-ac68-694af12ae24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Agora que temos nossos dados salvos no volume `catalogeddy_volume` vamos a carregar esses dados `CSV` em um `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00784866-5b5e-40aa-bb8e-ae670d6533dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, First Name: string, County: string, Sex: string, Count: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(f\"{path_volume}/{file_name}\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\",\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689703cb-569a-41b0-8292-616145cba73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, First Name: string, County: string, Sex: string, Count: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51b0a854-811b-46d2-bc15-7092d795713b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Agora, vamos salvar o `DataFrame` em uma tabela.\n",
    "\n",
    "Na seguinte célula vamos substituir um espaço no nome da coluna. [Caracteres especiais](https://docs.databricks.com/aws/pt/sql/language-manual/sql-ref-names), como espaços, não são permitidos nos nomes das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e6a27f-d4d8-4537-9f87-9b90b6911ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ano', 'Primeiro_Nome', 'Bairro', 'Sexo', 'Contagem']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"First Name\", \"First_Name\")\n",
    "\n",
    "df = df.withColumnRenamed(\"First_Name\", \"Primeiro_Nome\") \\\n",
    "       .withColumnRenamed(\"Year\", \"Ano\") \\\n",
    "           .withColumnRenamed(\"County\", \"Bairro\") \\\n",
    "       .withColumnRenamed(\"Sex\", \"Sexo\") \\\n",
    "       .withColumnRenamed(\"Count\", \"Contagem\")\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1410138-73d2-48ff-9be2-3edc3e5fb671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Ano: int, Primeiro_Nome: string, Bairro: string, Sexo: string, Contagem: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11f575c-3eac-4d4d-aadd-3ec9a1140476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando o DataFrame como Tabela:\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{path_table}\" + \".\" + f\"{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "import_query_and_data_visualization",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

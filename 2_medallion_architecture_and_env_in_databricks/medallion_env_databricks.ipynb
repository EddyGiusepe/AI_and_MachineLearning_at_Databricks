{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b1a6e3",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"><font color=\"gree\">Arquitetura Medallion e Ambiente de Desenvolvimento no Databricks</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ecee54",
   "metadata": {},
   "source": [
    "<font color=\"pink\">Senior Data Scientist.: Dr. Eddy Giusepe Chirinos Isidro</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba95b1",
   "metadata": {},
   "source": [
    "![](https://static.wixstatic.com/media/09d18d_aeac75794f8c4d089fa5266d142388d8~mv2.png/v1/fill/w_568,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/09d18d_aeac75794f8c4d089fa5266d142388d8~mv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259671f6",
   "metadata": {},
   "source": [
    "Na **Arquitetura Medallion**, cada camada tem um prop√≥sito:\n",
    "\n",
    "| Camada | Prop√≥sito | Quando Usar |\n",
    "|--------|-----------|-------------|\n",
    "| **Raw Data** | Arquivos brutos (CSV, JSON, etc.) | Primeira ingest√£o ou reprocessamento total |\n",
    "| **Bronze** ü•â | Dados brutos em formato otimizado (Delta) | Como fonte para transforma√ß√µes (Silver/Gold) |\n",
    "| **Silver** ü•à | Dados limpos e transformados | Para an√°lises e agrega√ß√µes |\n",
    "| **Gold** ü•á | Dados prontos para consumo (dashboards, ML) | Para usu√°rios finais |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e3cdb",
   "metadata": {},
   "source": [
    "# <font color=\"red\">logging</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4482d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logging() -> None:\n",
    "    \"\"\"Configures the logging system\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"LOGs_Databricks.log\"),\n",
    "            logging.StreamHandler(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "setup_logging()\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd3de7",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Conex√£o com o Databricks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84f5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Carregado minhas credenciais do Databricks, com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# S√≥ executar quando estiver trabalhando localmente:\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "databricks_host = os.environ['DATABRICKS_HOST']\n",
    "databricks_token = os.environ['DATABRICKS_TOKEN']\n",
    "databricks_cluster_id = os.environ['DATABRICKS_CLUSTER_ID']\n",
    "\n",
    "print(\"üîó Carregado minhas credenciais do Databricks, com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be551db6",
   "metadata": {},
   "source": [
    "A seguir vamos usar `Databricks Connect` para conectar ao Databricks e executar c√≥digo `Spark` localmente, mas processando os dados remotamente no cluster Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2725ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado ao Databricks com sucesso!\n",
      "üîß Vers√£o Spark: 4.0.0\n",
      "üì¶ DBUtils configurado e pronto para uso!\n"
     ]
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession # Esta classe √© usada para conectar ao Databricks remotamente\n",
    "from pyspark.dbutils import DBUtils # Para usar dbutils localmente\n",
    "\n",
    "spark = DatabricksSession.builder.remote(host=databricks_host, token=databricks_token, cluster_id=databricks_cluster_id).getOrCreate()\n",
    "\n",
    "# Criar inst√¢ncia do DBUtils para usar dbutils localmente:\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "print(\"‚úÖ Conectado ao Databricks com sucesso!\")\n",
    "print(f\"üîß Vers√£o Spark: {spark.version}\")\n",
    "print(f\"üì¶ DBUtils configurado e pronto para uso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e224be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar, s√≥ l√°, no Databricks para conhecer o ID do Cluster:\n",
    "\n",
    "#databricks_cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "\n",
    "#print(f\"Databricks Cluster ID: {databricks_cluster_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640c920",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Listando a ``Arquitetura Medallion`` com Unity Catalog</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea039bbd",
   "metadata": {},
   "source": [
    "No Unity Catalog, a hierarquia √©: ``catalog.schema.table``\n",
    "A seguir vamos visualizar/listar meus schemas e tabelas do catalog ``eddy_uber_taxi_v1``.\n",
    "\n",
    "Alguns comandos:\n",
    "\n",
    "```python\n",
    "SHOW CATALOGS        # --> Lista todos os cat√°logos\n",
    "SHOW SCHEMAS IN catalog_name    # --> Lista schemas de um cat√°logo\n",
    "SHOW TABLES IN catalog.schema   # --> Lista tabelas de um schema\n",
    "SHOW VOLUMES IN catalog.schema  # --> Lista volumes de um schema\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eedd7bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          catalog|\n",
      "+-----------------+\n",
      "|      catalogeddy|\n",
      "|eddy_uber_taxi_v1|\n",
      "|          samples|\n",
      "|           system|\n",
      "|        workspace|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogs = spark.sql(\"SHOW CATALOGS\").show() # Pode ser com .toPandas()\n",
    "\n",
    "catalogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389f216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|catalogeddy_schema|\n",
      "|information_schema|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meu_schema = spark.sql(\"SHOW SCHEMAS IN catalogeddy\").show()\n",
    "\n",
    "meu_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f921e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------+\n",
      "|          database|      tableName|isTemporary|\n",
      "+------------------+---------------+-----------+\n",
      "|catalogeddy_schema|tabel_baby_eddy|      false|\n",
      "+------------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meu_table = spark.sql(\"SHOW TABLES IN catalogeddy.catalogeddy_schema\").show()\n",
    "\n",
    "meu_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022795ba",
   "metadata": {},
   "source": [
    "A seguir vou trabalhar com meu catalog ``eddy_uber_taxi_v1``, j√° que eu sei que a√≠ tenho uma Arquitetura Medallion e vou listar meus ``schemas`` e ``tabelas``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea9d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|            bronze|\n",
      "|              gold|\n",
      "|information_schema|\n",
      "|          raw_data|\n",
      "|            silver|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalog_name = \"eddy_uber_taxi_v1\"\n",
    "\n",
    "schemas = spark.sql(f\"SHOW SCHEMAS IN {catalog_name}\").show() # Ou pode ser com .toPandas()\n",
    "\n",
    "schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e64e3687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:09,872 - __main__ - INFO - üèÜ TABELAS NO SCHEMA 'eddy_uber_taxi_v1.bronze'\n",
      "2025-10-19 17:01:10,710 - __main__ - INFO -   database tableName  isTemporary\n",
      "0   bronze  uber_raw        False\n",
      "2025-10-19 17:01:10,712 - __main__ - INFO - \n",
      "üìä Total de tabelas: 1\n",
      "2025-10-19 17:01:10,713 - __main__ - INFO - üèÜ TABELAS NO SCHEMA 'eddy_uber_taxi_v1.silver'\n",
      "2025-10-19 17:01:11,513 - __main__ - INFO -   database              tableName  isTemporary\n",
      "0   silver  uber_col_ptbr_silver1        False\n",
      "2025-10-19 17:01:11,516 - __main__ - INFO - \n",
      "üìä Total de tabelas: 1\n",
      "2025-10-19 17:01:11,517 - __main__ - INFO - üèÜ TABELAS NO SCHEMA 'eddy_uber_taxi_v1.gold'\n",
      "2025-10-19 17:01:12,413 - __main__ - WARNING - ‚ö†Ô∏è Nenhuma tabela encontrada\n"
     ]
    }
   ],
   "source": [
    "# Listar tabelas nos schemas 'bronze', 'silver' e 'gold':\n",
    "medallion_layers = ['bronze', 'silver', 'gold']\n",
    "\n",
    "for layer in medallion_layers:\n",
    "    logger.info(f\"üèÜ TABELAS NO SCHEMA '{catalog_name}.{layer}'\")\n",
    "    \n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{layer}\").toPandas()\n",
    "        if tables.empty:\n",
    "            logger.warning(\"‚ö†Ô∏è Nenhuma tabela encontrada\")\n",
    "        else:\n",
    "            logger.info(tables[['database', 'tableName', 'isTemporary']])\n",
    "            logger.info(f\"\\nüìä Total de tabelas: {len(tables)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"  ‚ùå Erro ao acessar schema '{layer}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8b8db",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Carregando dados a partir do ``bronze``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44825d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:15,311 - __main__ - INFO - O shape da tabela bronze √©: 200000 linhas e 9 colunas\n"
     ]
    }
   ],
   "source": [
    "catalog_name = \"eddy_uber_taxi_v1\"\n",
    "bronze_table = f\"{catalog_name}.bronze.uber_raw\"\n",
    "\n",
    "\n",
    "df_bronze = spark.table(bronze_table)\n",
    "\n",
    "logger.info(f\"O shape da tabela bronze √©: {df_bronze.count()} linhas e {len(df_bronze.columns)} colunas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7c4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:15,320 - __main__ - INFO - O schema da tabela bronze √©:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"O schema da tabela bronze √©:\\n\")\n",
    "df_bronze.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeee08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:15,342 - __main__ - INFO - üëÄ Preview dos dados (6 primeiras linhas):\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "|_c0     |key                |fare_amount|pickup_datetime    |pickup_longitude  |pickup_latitude  |dropoff_longitude |dropoff_latitude |passenger_count|\n",
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "|24238194|2015-05-07 19:52:06|7.5        |2015-05-07 19:52:06|-73.99981689453125|40.73835372924805|-73.99951171875   |40.72321701049805|1              |\n",
      "|27835199|2009-07-17 20:04:56|7.7        |2009-07-17 20:04:56|-73.994355        |40.728225        |-73.99471         |40.750325        |1              |\n",
      "|44984355|2009-08-24 21:45:00|12.9       |2009-08-24 21:45:00|-74.005043        |40.74077         |-73.962565        |40.772647        |1              |\n",
      "|25894730|2009-06-26 08:22:21|5.3        |2009-06-26 08:22:21|-73.976124        |40.790844        |-73.965316        |40.803349        |3              |\n",
      "|17610152|2014-08-28 17:47:00|16.0       |2014-08-28 17:47:00|-73.925023        |40.744085        |-73.97308199999999|40.761247        |5              |\n",
      "|44470845|2011-02-12 02:27:09|4.9        |2011-02-12 02:27:09|-73.96901899999999|40.75591         |-73.96901899999999|40.75591         |1              |\n",
      "+--------+-------------------+-----------+-------------------+------------------+-----------------+------------------+-----------------+---------------+\n",
      "only showing top 6 rows\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"üëÄ Preview dos dados (6 primeiras linhas):\\n\")\n",
    "\n",
    "df_bronze.show(6, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020386",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Renomeando Colunas do DataFrame para logo salvar no ``silver``</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351ff77",
   "metadata": {},
   "source": [
    "| M√©todo | Quando Usar | Pr√≥s | Contras |\n",
    "|--------|-------------|------|---------|\n",
    "| **1. withColumnRenamed()** | Poucas colunas (1-3) | Simples e leg√≠vel | Verboso para muitas colunas |\n",
    "| **2. Dicion√°rio + reduce()** | Muitas colunas, c√≥digo reutiliz√°vel | Elegante, f√°cil manuten√ß√£o | Requer importar `reduce` |\n",
    "| **3. toDF()** | Renomear TODAS as colunas | Muito r√°pido e direto | Precisa listar todas na ordem |\n",
    "| **4. select() + alias()** | Renomear + transformar dados | Flex√≠vel para transforma√ß√µes | Mais verboso |\n",
    "\n",
    "### ‚úÖ **Recomenda√ß√£o:**\n",
    "\n",
    "- **Para produ√ß√£o**: Use **M√©todo 2 (Dicion√°rio)** - mais limpo e f√°cil de manter\n",
    "- **Para prot√≥tipos r√°pidos**: Use **M√©todo 3 (toDF())** - mais r√°pido de escrever\n",
    "- **Se vai transformar dados tamb√©m**: Use **M√©todo 4 (select + alias)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3729ecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:18,151 - __main__ - INFO - üìä Colunas antigas: ['_c0', 'key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
      "2025-10-19 17:01:18,551 - __main__ - INFO - üìä Colunas novas:  ['id', 'chave_temporal', 'valor_corrida', 'data_hora_embarque', 'longitude_embarque', 'latitude_embarque', 'longitude_desembarque', 'latitude_desembarque', 'numero_passageiros']\n",
      "2025-10-19 17:01:18,552 - __main__ - INFO - \n",
      "üëÄ Preview dos dados:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------+-------------------+------------------+-----------------+---------------------+--------------------+------------------+\n",
      "|id      |chave_temporal     |valor_corrida|data_hora_embarque |longitude_embarque|latitude_embarque|longitude_desembarque|latitude_desembarque|numero_passageiros|\n",
      "+--------+-------------------+-------------+-------------------+------------------+-----------------+---------------------+--------------------+------------------+\n",
      "|24238194|2015-05-07 19:52:06|7.5          |2015-05-07 19:52:06|-73.99981689453125|40.73835372924805|-73.99951171875      |40.72321701049805   |1                 |\n",
      "|27835199|2009-07-17 20:04:56|7.7          |2009-07-17 20:04:56|-73.994355        |40.728225        |-73.99471            |40.750325           |1                 |\n",
      "|44984355|2009-08-24 21:45:00|12.9         |2009-08-24 21:45:00|-74.005043        |40.74077         |-73.962565           |40.772647           |1                 |\n",
      "|25894730|2009-06-26 08:22:21|5.3          |2009-06-26 08:22:21|-73.976124        |40.790844        |-73.965316           |40.803349           |3                 |\n",
      "|17610152|2014-08-28 17:47:00|16.0         |2014-08-28 17:47:00|-73.925023        |40.744085        |-73.97308199999999   |40.761247           |5                 |\n",
      "|44470845|2011-02-12 02:27:09|4.9          |2011-02-12 02:27:09|-73.96901899999999|40.75591         |-73.96901899999999   |40.75591            |1                 |\n",
      "+--------+-------------------+-------------+-------------------+------------------+-----------------+---------------------+--------------------+------------------+\n",
      "only showing top 6 rows\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce # Python padr√£o, n√£o PySpark\n",
    "\n",
    "# Mapeamento de nomes: ingl√™s -> portugu√™s\n",
    "colunas_ptbr = {\n",
    "    \"_c0\": \"id\",\n",
    "    \"key\": \"chave_temporal\",\n",
    "    \"fare_amount\": \"valor_corrida\",\n",
    "    \"pickup_datetime\": \"data_hora_embarque\",\n",
    "    \"pickup_longitude\": \"longitude_embarque\",\n",
    "    \"pickup_latitude\": \"latitude_embarque\",\n",
    "    \"dropoff_longitude\": \"longitude_desembarque\",\n",
    "    \"dropoff_latitude\": \"latitude_desembarque\",\n",
    "    \"passenger_count\": \"numero_passageiros\"\n",
    "}\n",
    "\n",
    "# Aplicar renomea√ß√£o:\n",
    "df_bronze_renamed = reduce(\n",
    "    lambda df, col: df.withColumnRenamed(col, colunas_ptbr[col]), # Fun√ß√£o\n",
    "    colunas_ptbr.keys(), # Iter√°vel\n",
    "    df_bronze) # Inicializador (opcional)\n",
    "\n",
    "logger.info(f\"üìä Colunas antigas: {df_bronze.columns}\")\n",
    "logger.info(f\"üìä Colunas novas:  {df_bronze_renamed.columns}\")\n",
    "\n",
    "logger.info(\"\\nüëÄ Preview dos dados:\")\n",
    "df_bronze_renamed.show(6, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55bb5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:20,209 - __main__ - INFO - üíæ Iniciando salvamento no Silver: eddy_uber_taxi_v1.silver.uber_col_ptbr_silver1\n",
      "2025-10-19 17:01:20,575 - __main__ - INFO - üìä Total de registros: 200,000\n",
      "2025-10-19 17:01:20,576 - __main__ - INFO - üìã Total de colunas: 9\n",
      "2025-10-19 17:01:25,093 - __main__ - INFO - ‚úÖ Tabela eddy_uber_taxi_v1.silver.uber_col_ptbr_silver1 salva com sucesso!\n",
      "2025-10-19 17:01:25,094 - __main__ - INFO - üîç Verificando tabela criada:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                  id|      int|   NULL|\n",
      "|      chave_temporal|timestamp|   NULL|\n",
      "|       valor_corrida|   double|   NULL|\n",
      "|  data_hora_embarque|timestamp|   NULL|\n",
      "|  longitude_embarque|   double|   NULL|\n",
      "|   latitude_embarque|   double|   NULL|\n",
      "|longitude_desemba...|   double|   NULL|\n",
      "|latitude_desembarque|   double|   NULL|\n",
      "|  numero_passageiros|      int|   NULL|\n",
      "|   data_uber_silver1|timestamp|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:01:26,368 - __main__ - INFO - üìä Total de registros salvos: 200,000\n"
     ]
    }
   ],
   "source": [
    "# Agora vou salvar este DataFrame \"df_bronze_renamed\" no \"silver\":\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "catalog_name = \"eddy_uber_taxi_v1\"\n",
    "schema_name = \"silver\"\n",
    "table_name = \"uber_col_ptbr_silver1\"\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "logger.info(f\"üíæ Iniciando salvamento no Silver: {full_table_name}\")\n",
    "logger.info(f\"üìä Total de registros: {df_bronze_renamed.count():,}\")\n",
    "logger.info(f\"üìã Total de colunas: {len(df_bronze_renamed.columns)}\")\n",
    "\n",
    "# Opcional: Adicionar metadados de auditoria:\n",
    "df_silver = df_bronze_renamed.withColumn(\"data_uber_silver1\", current_timestamp())\n",
    "\n",
    "# Salvar como tabela Delta:\n",
    "try:\n",
    "    df_silver.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "        .saveAsTable(full_table_name)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Tabela {full_table_name} salva com sucesso!\")\n",
    "    \n",
    "    # Verificar a tabela criada:\n",
    "    logger.info(f\"üîç Verificando tabela criada:\")\n",
    "    spark.sql(f\"DESCRIBE TABLE {full_table_name}\").show()\n",
    "    \n",
    "    # Contar registros salvos:\n",
    "    count = spark.table(full_table_name).count()\n",
    "    logger.info(f\"üìä Total de registros salvos: {count:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Erro ao salvar no Silver: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d981e5e",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Aplicando algumas ``Regras de Neg√≥cio``</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab92aa",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üìä REGRA DE NEG√ìCIO 1: Valida√ß√£o e Limpeza B√°sica</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615641ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:06:01,945 - __main__ - INFO - üîç Aplicando Regra de Neg√≥cio 1: Valida√ß√£o B√°sica\n",
      "2025-10-19 17:06:04,000 - __main__ - INFO - ‚úÖ Registros v√°lidos: 175,146\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"üîç Aplicando Regra de Neg√≥cio 1: Valida√ß√£o B√°sica\")\n",
    "\n",
    "# Usar nomes ORIGINAIS da tabela Bronze\n",
    "df_silver_valido = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        _c0 AS id,\n",
    "        key AS chave_temporal,\n",
    "        fare_amount AS valor_corrida,\n",
    "        pickup_datetime AS data_hora_embarque,\n",
    "        pickup_longitude AS longitude_embarque,\n",
    "        pickup_latitude AS latitude_embarque,\n",
    "        dropoff_longitude AS longitude_desembarque,\n",
    "        dropoff_latitude AS latitude_desembarque,\n",
    "        passenger_count AS numero_passageiros,\n",
    "        CURRENT_TIMESTAMP() AS data_processamento\n",
    "    FROM {catalog_name}.bronze.uber_raw\n",
    "    WHERE \n",
    "        -- Usar nomes ORIGINAIS nas condi√ß√µes\n",
    "        fare_amount > 0 \n",
    "        AND fare_amount < 500\n",
    "        \n",
    "        AND passenger_count > 0 \n",
    "        AND passenger_count <= 4\n",
    "        \n",
    "        AND pickup_longitude BETWEEN -74.05 AND -73.75\n",
    "        AND pickup_latitude BETWEEN 40.63 AND 40.85\n",
    "        AND dropoff_longitude BETWEEN -74.05 AND -73.75\n",
    "        AND dropoff_latitude BETWEEN 40.63 AND 40.85\n",
    "        \n",
    "        AND pickup_datetime <= CURRENT_TIMESTAMP()\n",
    "\"\"\")\n",
    "\n",
    "logger.info(f\"‚úÖ Registros v√°lidos: {df_silver_valido.count():,}\")\n",
    "\n",
    "# Salvar no Silver\n",
    "df_silver_valido.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{catalog_name}.silver.uber_valido\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b66678",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">üìä REGRA DE NEG√ìCIO 2: Deduplica√ß√£o e Padroniza√ß√£o</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1ec1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 18:10:38,634 - __main__ - INFO - üîç Aplicando Regra de Neg√≥cio 2: Deduplica√ß√£o\n",
      "2025-10-19 18:10:40,960 - __main__ - INFO - ‚úÖ Unique records: 199,269\n",
      "2025-10-19 18:10:47,456 - __main__ - INFO - ‚úÖ Table eddy_uber_taxi_v1.silver.uber_unique saved successfully!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"üîç Aplicando Regra de Neg√≥cio 2: Deduplica√ß√£o\")\n",
    "\n",
    "tb_silver_unico = spark.sql(f\"\"\"\n",
    "    WITH ranked_data AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY _c0 \n",
    "                ORDER BY pickup_datetime DESC\n",
    "            ) AS rank_temporal\n",
    "        FROM {catalog_name}.bronze.uber_raw\n",
    "        WHERE \n",
    "            fare_amount > 0 \n",
    "            AND passenger_count > 0\n",
    "    )\n",
    "    SELECT \n",
    "        _c0,\n",
    "        key,\n",
    "        ROUND(fare_amount, 2) AS fare_amount,\n",
    "        pickup_datetime,\n",
    "        DATE_FORMAT(pickup_datetime, 'dd/MM/yyyy HH:mm:ss') AS pickup_datetime_formatted,\n",
    "        ROUND(pickup_longitude, 4) AS pickup_longitude,\n",
    "        ROUND(pickup_latitude, 4) AS pickup_latitude,\n",
    "        ROUND(dropoff_longitude, 4) AS dropoff_longitude,\n",
    "        ROUND(dropoff_latitude, 4) AS dropoff_latitude,\n",
    "        passenger_count,\n",
    "        CURRENT_TIMESTAMP() AS processing_timestamp\n",
    "    FROM ranked_data\n",
    "    WHERE rank_temporal = 1\n",
    "\"\"\")\n",
    "\n",
    "logger.info(f\"‚úÖ Unique records: {tb_silver_unico.count():,}\")\n",
    "\n",
    "# Save to Silver\n",
    "tb_silver_unico.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{catalog_name}.silver.uber_unique_id_deduplicated\"\n",
    ")\n",
    "\n",
    "logger.info(f\"‚úÖ Table {catalog_name}.silver.uber_unique saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6faf09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
